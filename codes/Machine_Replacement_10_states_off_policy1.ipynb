{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import pickle \n",
    "import pandas as pd\n",
    "mp.set_start_method('spawn',True);\n",
    "import torch\n",
    "torch.multiprocessing.set_start_method('spawn',True);\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class Machine_Replacement:\n",
    "    def __init__(self,rep_cost=0.7,nS=6,nA=2):\n",
    "        self.nS = nS;\n",
    "        self.nA = nA;\n",
    "        #self.cost = np.linspace(1, 10000,nS);\n",
    "        #self.rep_cost = 7000;\n",
    "        self.cost = np.linspace(0.1, 0.99,nS);\n",
    "        self.rep_cost = rep_cost\n",
    "    def gen_probability(self):\n",
    "        self.P = np.zeros((self.nA,self.nS,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            for j in range(self.nS):\n",
    "                if(i<=j):\n",
    "                    self.P[0,i,j]=(2*(i+1))*((j+1)*(i+1));\n",
    "                else:\n",
    "                    continue;\n",
    "            self.P[0,i,:]=self.P[0,i,:]/np.sum(self.P[0,i,:])\n",
    "            self.P[1,i,0]=1;\n",
    "        return self.P;\n",
    "    def gen_reward(self):\n",
    "        self.R=np.zeros((self.nA,self.nS,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            self.R[0,i,:] = self.cost[i];\n",
    "            self.R[1,i,0] = self.rep_cost+self.cost[0];\n",
    "        return self.R;\n",
    "    def gen_expected_reward(self):\n",
    "        self.R = np.zeros((self.nA,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            self.R[0,i] = self.cost[i];\n",
    "            self.R[1,i] = self.rep_cost + self.cost[0];\n",
    "        return self.R;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 4 states. Ideal state was initialising alpha = 0.9, S=np.ones(nPOL) and F = np.ones(nPOL)*2; lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_hyperparameters:\n",
    "    def __init__(self):\n",
    "        self.T = 50000;\n",
    "        self.runs = 5;\n",
    "        self.lr = 0.01;\n",
    "        self.batch_size = 50;\n",
    "        self.start = 0;\n",
    "        self.nS = 4;\n",
    "        self.nA = 2;\n",
    "        self.rep_cost = 0.7\n",
    "        self.alpha = 0.9\n",
    "        self.gamma = 0.5\n",
    "        self.beta = 1;\n",
    "    \n",
    "    def ret_hyperparameters(self):\n",
    "        return (self.T,self.runs,self.lr,self.batch_size,self.start,self.nS,self.nA,self.rep_cost,self.alpha,self.gamma,self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weights(nn.Module):\n",
    "    def __init__(self,input_size,output_size,hidden_size = 0):\n",
    "        super(weights,self).__init__()\n",
    "        self.input_size = input_size;\n",
    "        self.hidden_size = hidden_size;\n",
    "        self.output_size = output_size;\n",
    "        if(hidden_size!=0):\n",
    "            self.linear1 = nn.Linear(self.input_size, self.hidden_size, bias=False)\n",
    "            self.linear2 = nn.Linear(self.hidden_size, self.output_size, bias=False)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(self.input_size, self.output_size, bias=False)\n",
    "        nn.init.ones_(self.linear1.weight)\n",
    "    '''\n",
    "        forward(): We accept a state 's' as input. Then we convert this into one hot encoding which is accomplished by first two lines.\n",
    "        Further we convert this one_hot vector 's' into pytorch tensor and then pass it through the network to obtain a output which is returned \n",
    "    '''\n",
    "    def forward(self,state):\n",
    "        s = np.zeros(self.input_size);\n",
    "        #print(state,end='===>');\n",
    "        s[state] = 1;\n",
    "        state = torch.FloatTensor(s).to(device)\n",
    "        #print(state);\n",
    "        if(self.hidden_size == 0):\n",
    "            output = torch.exp(self.linear1(state)) #To ensure that the outputs are always positive. giving Relu will cause problems.\n",
    "        else:\n",
    "            output = torch.exp(self.linear2(torch.exp(self.linear1(state))));\n",
    "        return output\n",
    "    \n",
    "    def forward_batch(self,states):\n",
    "        s = np.zeros((len(states),self.input_size));\n",
    "        #print(state,end='===>');\n",
    "        for i in range(len(states)):\n",
    "            s[i][states[i]] = 1\n",
    "        states = torch.FloatTensor(s).to(device)\n",
    "        #print(state);\n",
    "        if(self.hidden_size == 0):\n",
    "            output = torch.exp(self.linear1(states)) #To ensure that the outputs are always positive. giving Relu will cause problems.\n",
    "        else:\n",
    "            output = torch.exp(self.linear2(torch.exp(self.linear1(state))));\n",
    "        return output\n",
    "    \n",
    "    def fast_forward(self,s1,ns1,s2,ns2):\n",
    "        return self.forward(s1),self.forward(ns1),self.forward(s2),self.forward(ns2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target_Policy:\n",
    "    '''\n",
    "        First we create an initiualizer function namely a constructor to initialize the variables\n",
    "        with initial data values\n",
    "    '''\n",
    "    def __init__(self,S,A,P,R,start):\n",
    "        self.S=S # represant the states of the MDP\n",
    "        self.nS = len(S) # Reperesants the number of states of the MDP\n",
    "        self.nA = len(A);# Represants the number of actions in the MDP\n",
    "        self.P=P # Represants the true Probability function\n",
    "        self.R=R # Represants the true Reward  function\n",
    "        self.A=A;# Represnats the Action Space\n",
    "        self.K_pol = []; # To store all the policies\n",
    "        self.s_start=start # Store the start state \n",
    "    '''\n",
    "        In the generate_next_state(), we are generating our next state to be visited based on the following input parameters\n",
    "        s : Current state\n",
    "        a : Current action\n",
    "    '''    \n",
    "    def generate_next_state(self,s,a):\n",
    "        #p = np.zeros(self.nS)\n",
    "        p = self.P[a][s][:] # extrcat all the probabilities of the different statestransition given current s and a\n",
    "        #print(p);\n",
    "        return (np.argmax(np.random.multinomial(1,p)))\n",
    "    \n",
    "    '''\n",
    "        Single function to find the plot between the cumulative regret generated by different algorithms\n",
    "        Parameters:\n",
    "            reg_list : A list containing the regret value at different runs instances averaged over several time\n",
    "    '''    \n",
    "    def plot_data(self,reg_list):\n",
    "        plt.plot(np.arange(len(reg_list)),np.cumsum(np.array(reg_list)),marker = '+'); \n",
    "    '''\n",
    "        Function to find the optimum policy out of the K policies found out.\n",
    "        Parameters:\n",
    "            runs : To find for how many runs the current policy to be runned\n",
    "            T : Each run consisiting of how many time steps to find the average reward for each policy in one run\n",
    "            Time complexity : O(#(policies) x #(episode runs) x #(number of Time steps in one episode))\n",
    "    '''\n",
    "    def find_optimum_policy(self):\n",
    "        self.find_policies(); #Call the find_policies() to find all the policies and store it in 'self.K' list\n",
    "        final_R = np.zeros(len(self.K_pol));\n",
    "        for idx,pol in enumerate(self.K_pol):\n",
    "            #policy = self.one_hot(pol);\n",
    "            beh_obj = beh_pol_sd(self.P, pol, self.nS, self.nA)\n",
    "            state_distribution = beh_obj.state_distribution_simulated(1);\n",
    "            final_R[idx] = sum([state_distribution[state] *self.R[int(pol[state]),state] for state in range(self.nS)]);\n",
    "        for l_pol in range(len(self.K_pol)):\n",
    "            print(self.K_pol[l_pol],\"    ====>    \",final_R[l_pol]); # Display the the expected reward for each policy\n",
    "        return (final_R,self.K_pol[np.argmin(final_R)],np.min(final_R));# Return the minimum reward, the policy number which gives the minimum reward and the policy that gives minimum reward\n",
    "    \n",
    "    def find_policies(self):\n",
    "        self.K_pol = [];\n",
    "        pol=np.zeros(self.nS) # First policy is all 0's\n",
    "        self.K_pol.append(np.array(pol)); # append it to our K_policy list namely self.K\n",
    "        for i in reversed(range(self.nS)):\n",
    "            pol[i] = 1; # Come from the end and since the structure is thresholding nature so make each position 1 from 0 and append the same\n",
    "            print(pol);\n",
    "            self.K_pol.append(np.array(pol));\n",
    "        print(len(self.K_pol),\" policies found\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class average_case_distribution:\n",
    "    def __init__(self,nS,nA,behaviour_policy,state,lr,batch_size):\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        self.behaviour_policy = behaviour_policy;\n",
    "        self.state = state;\n",
    "        self.lr = lr\n",
    "        self.W_loss = 0\n",
    "        self.weight_obj = weights(nS,1).to(device);\n",
    "        self.W_loss = 0;\n",
    "        self.batch_size = batch_size\n",
    "    def set_target_policy(self,target_pol):\n",
    "        self.target_policy = target_pol;\n",
    "        self.optimizerW = optim.Adam(self.weight_obj.parameters(),lr = self.lr);\n",
    "        self.batch=[];\n",
    "    def show_policy(self):\n",
    "        print(self.target_policy);\n",
    "    def set_batch(self,data):\n",
    "        self.data = data;\n",
    "        self.T = len(data);\n",
    "    def get_batch(self):\n",
    "        if(self.T<=self.batch_size):\n",
    "            return self.data\n",
    "        else:\n",
    "            i = 1;\n",
    "            j=np.random.choice(self.T);\n",
    "            #j = np.random.randint(0,self.T,self.batch_size);\n",
    "            #batch = [[self.data[k][0],self.data[k][1],self.data[k][2]] for k in j]\n",
    "            batch=[];\n",
    "            while(i<=self.batch_size):\n",
    "                if(np.random.random()<=0.9):\n",
    "                    batch.append([self.data[j][0],self.data[j][1],self.data[j][2]])\n",
    "                    i+=1;\n",
    "                j = (j+1)%self.T;\n",
    "            return batch; \n",
    "    \n",
    "    def get_w(self,data,weight_obj,m,pair=0):\n",
    "        eps = torch.tensor(0.00000000001)\n",
    "        if(pair == 1):\n",
    "            Z_w_state = torch.tensor(0);\n",
    "            for i in range(len(data)):\n",
    "                val = weight_obj(data[i][0]);\n",
    "#                 print(val);\n",
    "                Z_w_state = torch.add(Z_w_state,val);\n",
    "            #print(Z_w_state.detach().numpy()[0]/self.batch_size);\n",
    "#             print((Z_w_state/self.batch_size)+eps)\n",
    "            return torch.add((Z_w_state/self.batch_size),eps);\n",
    "#             print(Z_w_state)\n",
    "        else:\n",
    "            state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2 = list(),list(),list(),list(),list(),list(),list(),list();\n",
    "            K = list();\n",
    "            for i in range(len(data)):\n",
    "                sample1 = data[i][0];\n",
    "                sample2 = data[i][1];\n",
    "                state1.append(sample1[0]);\n",
    "                #print(sample1);\n",
    "                w_state1.append(weight_obj(sample1[0]));\n",
    "                w_next_state1.append(weight_obj(sample1[2]));\n",
    "                state2.append(sample2[0]);\n",
    "                w_state2.append(weight_obj(sample2[0]));\n",
    "                w_next_state2.append(weight_obj(sample2[2]));\n",
    "                beta1.append(self.target_policy[sample1[0],sample1[1]]/self.behaviour_policy[sample1[0],sample1[1]]);\n",
    "                beta2.append(self.target_policy[sample2[0],sample2[1]]/self.behaviour_policy[sample2[0],sample2[1]]);\n",
    "                K.append(sample1[2]==sample2[2]);\n",
    "            return (state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2,K);\n",
    "    def get_w_updated(self,data,weight_obj,m,Z_w_state,pair=0): #data should be batch. note all inputs to be passed \n",
    "        if(pair == 1):\n",
    "            Z_w_state = 0;\n",
    "            for i in range(len(data)):\n",
    "                val = weight_obj(data[i][0]);\n",
    "                print(val);\n",
    "                Z_w_state+=val;\n",
    "            #print(Z_w_state.detach().numpy()[0]/self.batch_size);\n",
    "            Z_w_state = Z_w_state.item()/self.batch_size;\n",
    "            if(Z_w_state<0.00000000000005):\n",
    "                Z_w_state+=0.000000000001;\n",
    "            return Z_w_state;\n",
    "        else:\n",
    "            data = torch.tensor(data)\n",
    "#             print(data[:,0])\n",
    "#             print(data[:,2])\n",
    "#             print(data[:,0].size())\n",
    "            new_data_current = torch.reshape(data[:,0],(self.batch_size,1))\n",
    "#             w_current_state = weight_obj.forward_batch(new_data_current)\n",
    "            \n",
    "            new_data_next = torch.reshape(data[:,2],(self.batch_size,1))\n",
    "            new_data = torch.cat((new_data_current,new_data_next),0)\n",
    "            w = weight_obj.forward_batch(new_data)\n",
    "#             print(w)\n",
    "#             print(Z_w_state)\n",
    "#             print(w/Z_w_state)\n",
    "#             print(\"****\")\n",
    "            w = w/Z_w_state\n",
    "#             print(w)\n",
    "#             print(new_data.size())\n",
    "#             print(w.size())\n",
    "            w = torch.reshape(w,(2,self.batch_size))\n",
    "            beta = torch.zeros(self.batch_size,1)\n",
    "            for i in range(len(data)):\n",
    "                beta[i] = self.target_policy[data[i][0],data[i][1]]/self.behaviour_policy[data[i][0],data[i][1]]\n",
    "            \n",
    "            K = torch.zeros(self.batch_size,self.batch_size)\n",
    "            for i in range(len(data)):\n",
    "                for j in range(len(data)):\n",
    "                    K[i][j] = (data[i][2] == data[j][2])\n",
    "#             print(\"here\")\n",
    "#             print(w.size())\n",
    "#             print(beta.size())\n",
    "#             print(K.size())\n",
    "            w_0 = torch.reshape(w[0], (self.batch_size,1))\n",
    "            w_1 = torch.reshape(w[1], (self.batch_size,1))\n",
    "#             print(w_0)\n",
    "#             print(beta)\n",
    "#             print(w_1)\n",
    "#             print(w_0 * beta - w_1)\n",
    "#check here only\n",
    "            delta = w_0 * beta - w_1\n",
    "            temp1 = torch.matmul(K,delta) #50,50:50,1 = 50, 1\n",
    "            final_sum = torch.matmul(torch.transpose(delta,0,1),temp1)\n",
    "#             print(w_0)\n",
    "#             print(w_1)\n",
    "#             print(Z_w_state)\n",
    "#             print(final_sum)\n",
    "#             print(\".........\")\n",
    "            return final_sum\n",
    "    def updated_update_state_distribution_ratio(self):\n",
    "        batch = self.get_batch();\n",
    "        eps = 0.04;\n",
    "        #self.data_used[run] =self.data_used[run]+batch;\n",
    "        #self.selected_policy = selected_policy\n",
    "        pairs = list(product(batch,repeat=2));\n",
    "        self.loss_episode = [];\n",
    "        for _ in range(500):\n",
    "            batch = self.get_batch();\n",
    "            pairs = list(product(batch,repeat=2));\n",
    "            state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2,K = self.get_w(pairs, self.weight_obj, len(batch));\n",
    "            Z_w_state = self.get_w(batch, self.weight_obj, len(batch),1);\n",
    "            self.w_loss = 0\n",
    "            for i in range(len(state1)):\n",
    "                self.w_loss+=(beta1[i]*(w_state1[i]/Z_w_state) - (w_next_state1[i]/Z_w_state))*(beta2[i]*(w_state2[i]/Z_w_state)-(w_next_state2[i]/Z_w_state))*K[i];\n",
    "            self.w_loss/=(self.batch_size);\n",
    "            self.optimizerW.zero_grad();\n",
    "            self.w_loss.backward();\n",
    "            self.optimizerW.step();\n",
    "            self.optimizerW.zero_grad();\n",
    "            #self.Z.append(Z_w_state)\n",
    "        #self.loss.append(self.w_loss.cpu().detach().numpy()[0]);\n",
    "        state_dist=[];\n",
    "        for i in range(self.nS):\n",
    "            w_state = self.weight_obj(i);\n",
    "            w_state = w_state.item();\n",
    "            state_dist.append(w_state);\n",
    "        return np.array(state_dist);\n",
    "    def update_state_distribution_ratio(self):\n",
    "        self.batch = self.get_batch();\n",
    "        batch=self.batch;\n",
    "        temp = 0\n",
    "        #eps = 0.0004;\n",
    "        for _ in range(5):\n",
    "             start = time.time()\n",
    "             batch = self.get_batch();\n",
    "#             pairs = product(batch,repeat=2)\n",
    "             Z_w_state = self.get_w(batch, self.weight_obj, len(batch),1);\n",
    "#             state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2,K = self.get_w(pairs, self.weight_obj, len(batch));\n",
    "             self.w_loss = 0\n",
    "             self.w_loss = self.get_w_updated(batch,self.weight_obj,len(batch),Z_w_state)\n",
    "#             print(time.time() - start)\n",
    "#             print(\"*******\")\n",
    "#             for i in range(len(state1)):\n",
    "#                 self.w_loss+=(beta1[i]*(w_state1[i]/Z_w_state) - (w_next_state1[i]/Z_w_state))*(beta2[i]*(w_state2[i]/Z_w_state)-(w_next_state2[i]/Z_w_state))*K[i];\n",
    "#             print(\"sourav\")\n",
    "#             print(self.w_loss)\n",
    "             self.w_loss/=(self.batch_size*self.batch_size);\n",
    "             temp += self.w_loss\n",
    "             start = time.time()\n",
    "             self.optimizerW.zero_grad();\n",
    "             self.w_loss.backward(); #Improving the forward pass computation\n",
    "             self.optimizerW.step();\n",
    "             self.optimizerW.zero_grad();\n",
    "             if(self.batch_size==self.T):\n",
    "                    break;\n",
    "        state_dist=[];\n",
    "        for i in range(self.nS):\n",
    "            w_state = self.weight_obj(i);\n",
    "            w_state = w_state.item();\n",
    "            state_dist.append(w_state);\n",
    "        return np.array(state_dist);\n",
    "#             print(time.time() - start)\n",
    "#            self.Z.append(Z_w_state)\n",
    "            \n",
    "#        self.loss.append(self.w_loss.cpu().detach().numpy()[0]);\n",
    "#        state_dist=[];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class beh_pol_sd:\n",
    "    def __init__(self,P,policy,nS,nA):\n",
    "        self.P = P\n",
    "        self.policy = policy\n",
    "        self.nS = nS;\n",
    "        self.nA = nA;\n",
    "    \n",
    "    def onehot(self):\n",
    "        pol = np.zeros((self.nS,self.nA));\n",
    "        for i in range(self.nS):\n",
    "            pol[i][int(self.policy[i])]=1;\n",
    "        return pol;\n",
    "    def find_transition_matrix(self,onehot_encode=1):\n",
    "        if(onehot_encode==1):\n",
    "            self.policy = self.onehot()\n",
    "        T_s_s_next = np.zeros((self.nS,self.nS));\n",
    "        for s in range(self.nS):\n",
    "            for s_next in range(self.nS):\n",
    "                for a in range(self.nA):\n",
    "                    #print(s,s_next,a);\n",
    "                    #print(T[a,s,s_next]);\n",
    "                    T_s_s_next[s,s_next]+=self.P[a,s,s_next]*self.policy[s,a];\n",
    "        return T_s_s_next;\n",
    "    def state_distribution_simulated(self,onehot_encode=1):\n",
    "        P_policy = self.find_transition_matrix(onehot_encode)\n",
    "        #print(P_policy);\n",
    "        P_dash = np.append(P_policy - np.eye(self.nS),np.ones((self.nS,1)),axis=1);\n",
    "        #print(P_dash);\n",
    "        P_last = np.linalg.pinv(np.transpose(P_dash))[:,-1]\n",
    "        return P_last;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(target_policy,nS,nA):\n",
    "  nPOL = len(target_policy);\n",
    "  one_hot_target_policy = []\n",
    "  for i in range(nPOL):\n",
    "    pol=np.zeros((nS,nA));\n",
    "    for s in range(nS):\n",
    "        pol[s,target_policy[i,s]]=1\n",
    "    one_hot_target_policy.append(pol);\n",
    "  return np.array(one_hot_target_policy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episode(T,state,behaviour_policy,P,batch_size,run):\n",
    "  #global P,behaviour_policy,batch_size;\n",
    "  data={};temp=[];\n",
    "  for t in range(1,T+1): \n",
    "    action = np.argmax(np.random.multinomial(1,behaviour_policy[state,:]))\n",
    "    next_state = np.argmax(np.random.multinomial(1,P[action,state,:]));\n",
    "    temp.append([state,action,next_state]);\n",
    "    state = next_state;\n",
    "    if(t%batch_size==0):\n",
    "      #print(t);\n",
    "      data[int(t/batch_size)-1] = temp[:];\n",
    "  #print(len(data));\n",
    "  with open('Data_used_'+str(run),'wb') as f:\n",
    "    pickle.dump(temp,f);\n",
    "  f.close();\n",
    "  return data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "T,runs,lr,batch_size,start,nS,nA,rep_cost,alpha,gamma,beta = get_hyperparameters().ret_hyperparameters();\n",
    "mr_obj = Machine_Replacement(0.7,nS,nA)\n",
    "P = mr_obj.gen_probability()\n",
    "R = mr_obj.gen_expected_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52631579 0.05921053 0.11303828 0.30143541]\n"
     ]
    }
   ],
   "source": [
    "nPOL = nS-1\n",
    "behaviour_policy = np.ones((nS,nA))*0.5\n",
    "behaviour_policy_state_distribution = beh_pol_sd(P,behaviour_policy,nS,nA).state_distribution_simulated(0);\n",
    "print(behaviour_policy_state_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1]\n",
      " [0 0 1 1]\n",
      " [0 1 1 1]]\n",
      "[0.35308953 0.09079445 0.23833544 0.31778058]\n",
      "[0.46357616 0.1192053  0.17880795 0.2384106 ]\n",
      "[0.52631579 0.10526316 0.15789474 0.21052632]\n"
     ]
    }
   ],
   "source": [
    "target_policy = np.zeros((nPOL,nS),dtype = np.int8)\n",
    "for i in range(nPOL,0,-1):\n",
    "    target_policy[nPOL-i][-(nPOL-i+1):] = 1\n",
    "print(target_policy)\n",
    "for pol in target_policy:\n",
    "    dist = beh_pol_sd(P,pol,nS,nA).state_distribution_simulated(1)\n",
    "    print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████                    | 500/1000 [02:02<02:05,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => [0.3792091  0.08724087 0.22059296 0.31295707]\n",
      "[0.47583656 0.41430797 0.43789966]\n",
      "1 => [0.4895032  0.10319922 0.15634317 0.25095441]\n",
      "[0.47583656 0.41572407 0.43789966]\n",
      "2 => [0.52712627 0.09384142 0.15401095 0.22502136]\n",
      "[0.47583656 0.41572407 0.43101161]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [04:03<00:00,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => [0.38676223 0.09022126 0.21537948 0.30763703]\n",
      "[0.46990339 0.43259923 0.43305229]\n",
      "1 => [0.46399176 0.10614449 0.17329298 0.25657077]\n",
      "[0.46990339 0.43239416 0.43305229]\n",
      "2 => [0.52533772 0.0924042  0.14948252 0.23277555]\n",
      "[0.46990339 0.43239416 0.43226359]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|████████████████████                    | 500/1000 [01:58<01:58,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => [0.35518177 0.09749486 0.23893777 0.3083856 ]\n",
      "[0.48656314 0.42158843 0.43675531]\n",
      "1 => [0.48018535 0.10463643 0.15647229 0.25870593]\n",
      "[0.48656314 0.4216669  0.43675531]\n",
      "2 => [0.52428065 0.09433725 0.15408199 0.22730011]\n",
      "[0.48656314 0.4216669  0.43300355]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [03:57<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => [0.36649097 0.09583738 0.23134825 0.3063234 ]\n",
      "[0.48012476 0.42605458 0.42822009]\n",
      "1 => [0.47228932 0.10981137 0.17021691 0.2476824 ]\n",
      "[0.48012476 0.42510689 0.42822009]\n",
      "2 => [0.53379214 0.09417927 0.15968147 0.21234711]\n",
      "[0.48012476 0.42510689 0.4263455 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|████████████████████                    | 500/1000 [01:59<01:57,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => [0.35915481 0.09474402 0.22863669 0.31746449]\n",
      "[0.4859903  0.43084508 0.4372022 ]\n",
      "1 => [0.46693301 0.10980422 0.17238661 0.25087616]\n",
      "[0.4859903  0.42885919 0.4372022 ]\n",
      "2 => [0.52595626 0.09950127 0.15378577 0.22075669]\n",
      "[0.4859903  0.42885919 0.43183062]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [03:59<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => [0.38849353 0.09344365 0.21588926 0.30217356]\n",
      "[0.4673374  0.42726909 0.43500759]\n",
      "1 => [0.47372839 0.1037535  0.17555062 0.24696749]\n",
      "[0.4673374  0.42654288 0.43500759]\n",
      "2 => [0.52144372 0.09501916 0.14967209 0.23386502]\n",
      "[0.4673374  0.42654288 0.4349894 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|████████████████████                    | 500/1000 [02:00<02:00,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => [0.37874761 0.09359694 0.21217502 0.31548042]\n",
      "[0.4744939  0.42808937 0.42937359]\n",
      "1 => [0.4759367  0.10157842 0.16808708 0.2543978 ]\n",
      "[0.4744939  0.42587435 0.42937359]\n",
      "2 => [0.52688184 0.10443154 0.15300795 0.21567867]\n",
      "[0.4744939  0.42587435 0.43118271]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [04:00<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => [0.38117028 0.08965238 0.22066614 0.3085112 ]\n",
      "[0.47348329 0.41828977 0.44354943]\n",
      "1 => [0.48543352 0.10607748 0.16667342 0.24181559]\n",
      "[0.47348329 0.41741196 0.44354943]\n",
      "2 => [0.51170918 0.09344923 0.15501578 0.23982581]\n",
      "[0.47348329 0.41741196 0.44180357]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|████████████████████                    | 500/1000 [01:56<01:56,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => [0.38174738 0.09602476 0.2105066  0.31172125]\n",
      "[0.47159281 0.41902    0.4522406 ]\n",
      "1 => [0.48421882 0.10416407 0.16994118 0.24167593]\n",
      "[0.47159281 0.41903399 0.4522406 ]\n",
      "2 => [0.50673698 0.10283328 0.16326218 0.22716756]\n",
      "[0.47159281 0.41903399 0.44528412]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [03:52<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 => [0.37824982 0.09734162 0.22935572 0.29505284]\n",
      "[0.47149939 0.42034713 0.43427566]\n",
      "1 => [0.47722009 0.1058692  0.17099934 0.24591137]\n",
      "[0.47149939 0.42324536 0.43427566]\n",
      "2 => [0.52750466 0.10163908 0.14810891 0.22274735]\n",
      "[0.47149939 0.42324536 0.43074674]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "one_hot_target_policy = one_hot(target_policy,nS,nA)\n",
    "policy_sampled=np.zeros((int(T/batch_size),runs))\n",
    "value_sampled=np.zeros((int(T/batch_size),runs))\n",
    "for run in range(runs):\n",
    "    vf_arr = np.zeros(nPOL)\n",
    "    data = simulate_episode(T,start,behaviour_policy,P,batch_size,0);\n",
    "    all_network=dict();\n",
    "    sampled_policy = 0;\n",
    "    rew_vect=np.zeros(nPOL)\n",
    "    n=np.zeros(nPOL)\n",
    "    for i in range(len(one_hot_target_policy)):\n",
    "        all_network[i] = average_case_distribution(nS,nA,behaviour_policy,start,lr,batch_size)\n",
    "        all_network[i].set_target_policy(one_hot_target_policy[i]);\n",
    "    for t in tqdm(range(1,int(T/batch_size)+1)):\n",
    "        for i in range(nPOL):\n",
    "            all_network[i].set_batch(data[t-1]);\n",
    "            c=all_network[i].update_state_distribution_ratio()*behaviour_policy_state_distribution;\n",
    "            sd=c/np.sum(c);\n",
    "            rew_vect[i] = sum([sd[state] *R[int(target_policy[i,state]),state] for state in range(nS)]);\n",
    "            if(t%500==0):\n",
    "                print(i,\"=>\",sd)\n",
    "                print(rew_vect)\n",
    "        sampled_policy = np.argmin(rew_vect);\n",
    "        value_sampled[t-1,run] = rew_vect[sampled_policy]\n",
    "        n[sampled_policy]+=1;\n",
    "        policy_sampled[t-1,run] = sampled_policy\n",
    "    \n",
    "pd.DataFrame(policy_sampled).to_excel(\"Policy_sampled_off_policy1_\"+str(nS)+\"_states.xlsx\")\n",
    "#[0.46147503 0.01742633 0.02773987 0.04027418 0.05034273 0.06041128 0.07047982 0.08054837 0.09061692 0.10068546]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "[0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "11  policies found\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]     ====>     0.9900000000000165\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]     ====>     0.5980026712414057\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]     ====>     0.5152783499018663\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]     ====>     0.4731485120073595\n",
      "[0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]     ====>     0.4503924838042788\n",
      "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]     ====>     0.4394920053578094\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]     ====>     0.4363168141033407\n",
      "[0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]     ====>     0.43798070324723415\n",
      "[0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]     ====>     0.4421537396121882\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]     ====>     0.446788990825688\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]     ====>     0.7999999999999997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.99      , 0.59800267, 0.51527835, 0.47314851, 0.45039248,\n",
       "        0.43949201, 0.43631681, 0.4379807 , 0.44215374, 0.44678899,\n",
       "        0.8       ]),\n",
       " array([0., 0., 0., 0., 1., 1., 1., 1., 1., 1.]),\n",
       " 0.4363168141033407)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp=Target_Policy(np.arange(nS),np.arange(nA),P,R,0)\n",
    "tp.find_optimum_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51172349, 0.01442877, 0.02128451, 0.03497395, 0.04241495,\n",
       "       0.0457826 , 0.06536528, 0.07298361, 0.08121001, 0.10983283])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       ...,\n",
       "       [5.],\n",
       "       [6.],\n",
       "       [6.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.where(policy_sampled==3,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6+5+4+7+8+3\n",
    "657+415+600+200+67+60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
